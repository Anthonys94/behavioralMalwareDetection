import tensorflow as tf
from tensorflow import keras
import keras.backend as K
from IPython.display import display, HTML

   
"""
    At the current state, our ExplainableLSTM works only if the input model
    has the following layers at its end:
        Embedding -> LSTM -> Dense (Linear Activation) -> Activation (Softmax)
"""
    
class ExplainableLSTM_LRP():

    def __init__(self, model_or_path):
        if isinstance(model_or_path, str):
            self.model = keras.models.load_model(model_or_path)
        else:
            self.model = model_or_path

        _check_model_structure(self.model.layers)

        self.layers = _set_layers(self.model)
        self.weights = _set_weights(self.layers)

        self.EMBEDDING_SIZE = self.weights['embedding'].shape[1]
        self.LSTM_UNITS = int(int(self.weights['lstm']['W'].shape[1]) / 4)
        self.C = self.weights['result']['W'].shape[1]
        print("A")

    """
    Arguments:
        x: input sequence formed by indices of words in a vocabulary
           e.g. [12 15 23 43 24 24 24]
        
        LRP_CLASS: class we want to explain

        EPS, DELTA: parameters of LRP
    
    Result:
        scores: LRP scores for each input word

    """
    def explain_instance(self, x, LRP_CLASS, EPS, DELTA, verbose = False):
        T = len(x) # sequence length
        LSTM_UNITS = self.LSTM_UNITS # for simplicity...
        EMBEDDING_SIZE = self.EMBEDDING_SIZE
        C = self.C
        weights = self.weights

        # Indices of LSTM gates as defined by Keras implementation
        I, F, G, O = np.arange(0,LSTM_UNITS), np.arange(LSTM_UNITS,2*LSTM_UNITS), np.arange(2*LSTM_UNITS,3*LSTM_UNITS), np.arange(3*LSTM_UNITS,4*LSTM_UNITS) 

        # Network forward
        embeddings = self.layers['embedding'](K.expand_dims(x, axis = 0))

        h, c, gates_pre, gates = _lstm_forward(embeddings[0], weights, T, LSTM_UNITS)
        lstm_output = K.expand_dims(tf.convert_to_tensor(h[-2]), axis=0)

        hout_result = self.layers['result'](lstm_output)

        # LRP Initializations
        Rx = np.zeros(embeddings[0].shape)
        Rh = np.zeros((T+1, LSTM_UNITS)) # the "-1" position will contain the "rest" relevances
        Rc = np.zeros((T+1, LSTM_UNITS)) # //
        Rg = np.zeros((T, LSTM_UNITS))
        Rout_mask            = np.zeros((C))
        Rout_mask[LRP_CLASS] = 1.0         


        # Last Dense Layer relevance propagation
        Rh[T-1] = lrp_linear(
            hin = h[T-1],
            w = weights['result']['W'],
            b = np.zeros((C)),
            hout = hout_result[0].numpy(),
            Rout = hout_result[0].numpy() * Rout_mask,
            bias_nb_units = LSTM_UNITS,
            eps = EPS,
            bias_factor = DELTA,
            debug = False    
        )

        # Propagation through the sequence of LSTM cells
        for t in reversed(range(T)):
            Rc[t]   += Rh[t]
            #                       hin                             W                           b                       hout           Rout         bias_nb_units
            Rc[t-1]  = lrp_linear(gates[t,F]*c[t-1]         , np.identity(LSTM_UNITS)   , np.zeros((LSTM_UNITS))    , c[t]           , Rc[t], 2*LSTM_UNITS  , EPS, DELTA, debug=False)
            Rg[t]    = lrp_linear(gates[t,I]*gates[t,G]     , np.identity(LSTM_UNITS)   , np.zeros((LSTM_UNITS))    , c[t]           , Rc[t], 2*LSTM_UNITS  , EPS, DELTA, debug=False)
            Rx[t]    = lrp_linear(embeddings[0][t].numpy()  , weights['lstm']['W'][:,G] , weights['lstm']['b'][G]   , gates_pre[t, G], Rg[t], LSTM_UNITS+EMBEDDING_SIZE  , EPS, DELTA, debug=False)
            Rh[t-1]  = lrp_linear(h[t-1]                    , weights['lstm']['U'][:,G] , weights['lstm']['b'][G]   , gates_pre[t, G], Rg[t], LSTM_UNITS+EMBEDDING_SIZE  , EPS, DELTA, debug=False)

            if verbose == True:
                print("\nt = ", t)
                print("---Rh[", t, "] = ", Rh[t].sum())
                print("---Rc[", t, "] = ", Rc[t].sum())
                print("------Rc[", t-1, "] = ", Rc[t-1].sum())
                print("------Rg[", t, "] = ", Rg[t].sum())
                print("------check_sum = ", Rc[t-1].sum() + Rg[t].sum(), " | ", np.allclose(Rc[t-1].sum() + Rg[t].sum(), Rc[t].sum()))
                print("---------Rx[", t, "] = ", Rx[t].sum())
                print("---------Rh[", t-1, "] = ", Rh[t-1].sum())
                print("---------check_sum = ", Rh[t-1].sum() + Rx[t].sum(), " | ", np.allclose(Rh[t-1].sum() + Rx[t].sum(), Rg[t].sum()))

        scores = Rx.sum(axis = 1)

        if verbose is True:
            print("\nProbability of class ", LRP_CLASS, " is ", self.model.predict(x.reshape(1,-1))[0][LRP_CLASS])
            print("Relevance score to be propagated: ", (hout_result[0].numpy() * Rout_mask).sum())

        return scores, Rx, Rh, Rg, Rc

    def show_in_notebook(self, x, scores):
        display(HTML(html_heatmap(x, scores)))


def _check_model_structure(layers):
        assert isinstance(layers[-1], tf.python.keras.layers.core.Activation)
        assert isinstance(layers[-2], tf.python.keras.layers.core.Dense)
        assert isinstance(layers[-3], tf.python.keras.layers.recurrent_v2.LSTM)
        assert isinstance(layers[-4], tf.python.keras.layers.embeddings.Embedding)

def _set_weights(layers):
    weights = {}
    weights['embedding'] = layers['embedding'].embeddings.numpy()
    weights['lstm'] = {'W': layers['lstm'].weights[0].numpy(),
                       'U': layers['lstm'].weights[1].numpy(),
                       'b': layers['lstm'].weights[2].numpy()}
    
    weights['result'] = {}
    weights['result']['W'] = layers['result'].weights[0].numpy()
    try: 
        # This raw gives an error if the last Dense layer has use_bias = False
        weights['result']['b'] = layers['result'].weights[1].numpy()
    except:
        print("[!] - Last layer does not have bias")
    return weights

def _set_layers(model):
    layers = {'embedding': model.layers[-4],
              'lstm':      model.layers[-3],
              'result':    model.layers[-2]}
    return layers
    
def _lstm_forward(x, weights, T, LSTM_UNITS):            

    # gate indices (assuming the gate ordering in the LSTM weights is i,f,c,o):     
    idx    = np.hstack((np.arange(0,2*LSTM_UNITS), 
                        np.arange(3*LSTM_UNITS,4*LSTM_UNITS))).astype(int) # indices of gates i,f,o together

    idx_i, idx_f, idx_g, idx_o = np.arange(0,LSTM_UNITS), np.arange(LSTM_UNITS,2*LSTM_UNITS), np.arange(2*LSTM_UNITS,3*LSTM_UNITS), np.arange(3*LSTM_UNITS,4*LSTM_UNITS) # indices of gates i,g,f,o separately
     
    # initialize
    h         = np.zeros((T+1, LSTM_UNITS))
    c         = np.zeros((T+1, LSTM_UNITS))

    gates_xh  = np.zeros((T, 4*LSTM_UNITS))  
    gates_hh  = np.zeros((T, 4*LSTM_UNITS)) 
    gates_pre = np.zeros((T, 4*LSTM_UNITS))  # gates pre-activation
    gates     = np.zeros((T, 4*LSTM_UNITS))  # gates activation
         
    for t in range(T): 
        gates_xh[t]     = np.dot(weights['lstm']['W'].T, x[t])        
        gates_hh[t]     = np.dot(weights['lstm']['U'].T, h[t-1]) 
        gates_pre[t]    = gates_xh[t] + gates_hh[t] + weights['lstm']['b']
        gates[t,idx]    = 1.0/(1.0 + np.exp(- gates_pre[t,idx]))
        gates[t,idx_g]  = np.tanh(gates_pre[t,idx_g]) 
        c[t]            = gates[t,idx_f]*c[t-1] + gates[t,idx_i]*gates[t,idx_g]
        h[t]            = gates[t,idx_o]*np.tanh(c[t])
    
    return h, c, gates_pre, gates # prediction scores

'''
@author: Leila Arras
@maintainer: Leila Arras
@date: 21.06.2017
@version: 1.0
@copyright: Copyright (c) 2017, Leila Arras, Gregoire Montavon, Klaus-Robert Mueller, Wojciech Samek
@license: BSD-2-Clause
'''

import numpy as np
from numpy import newaxis as na


def lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor=0.0, debug=False):
    """
    LRP for a linear layer with input dim D and output dim M.
    Args:
    - hin:            forward pass input, of shape (D,)
    - w:              connection weights, of shape (D, M)
    - b:              biases, of shape (M,)
    - hout:           forward pass output, of shape (M,) (unequal to np.dot(w.T,hin)+b if more than one incoming layer!)
    - Rout:           relevance at layer output, of shape (M,)
    - bias_nb_units:  total number of connected lower-layer units (onto which the bias/stabilizer contribution is redistributed for sanity check)
    - eps:            stabilizer (small positive number)
    - bias_factor:    set to 1.0 to check global relevance conservation, otherwise use 0.0 to ignore bias/stabilizer redistribution (recommended)
    Returns:
    - Rin:            relevance at layer input, of shape (D,)
    """
    sign_out = np.where(hout[na,:]>=0, 1., -1.) # shape (1, M)
    
    numer    = (w * hin[:,na]) + ( bias_factor * (b[na,:]*1. + eps*sign_out*1.) / bias_nb_units ) # shape (D, M)
    # Note: here we multiply the bias_factor with both the bias b and the stabilizer eps since in fact
    # using the term (b[na,:]*1. + eps*sign_out*1.) / bias_nb_units in the numerator is only useful for sanity check
    # (in the initial paper version we were using (bias_factor*b[na,:]*1. + eps*sign_out*1.) / bias_nb_units instead)
    
    denom    = hout[na,:] + (eps*sign_out*1.)   # shape (1, M)
    
    message  = (numer/denom) * Rout[na,:]       # shape (D, M)
    
    Rin      = message.sum(axis=1)              # shape (D,)
    
    if debug:
        print("local diff: ", Rout.sum() - Rin.sum())
    # Note: 
    # - local  layer   relevance conservation if bias_factor==1.0 and bias_nb_units==D (i.e. when only one incoming layer)
    # - global network relevance conservation if bias_factor==1.0 and bias_nb_units set accordingly to the total number of lower-layer connections 
    # -> can be used for sanity check
    
    return Rin

import matplotlib.pyplot as plt

def rescale_score_by_abs (score, max_score, min_score):
    """
    Normalize the relevance value (=score), accordingly to the extremal relevance values (max_score and min_score), 
    for visualization with a diverging colormap.
    i.e. rescale positive relevance to the range [0.5, 1.0], and negative relevance to the range [0.0, 0.5],
    using the highest absolute relevance for linear interpolation.
    """
    
    # CASE 1: positive AND negative scores occur --------------------
    if max_score>0 and min_score<0:
    
        if max_score >= abs(min_score):   # deepest color is positive
            if score>=0:
                return 0.5 + 0.5*(score/max_score)
            else:
                return 0.5 - 0.5*(abs(score)/max_score)

        else:                             # deepest color is negative
            if score>=0:
                return 0.5 + 0.5*(score/abs(min_score))
            else:
                return 0.5 - 0.5*(score/min_score)   
    
    # CASE 2: ONLY positive scores occur -----------------------------       
    elif max_score>0 and min_score>=0: 
        if max_score == min_score:
            return 1.0
        else:
            return 0.5 + 0.5*(score/max_score)
    
    # CASE 3: ONLY negative scores occur -----------------------------
    elif max_score<=0 and min_score<0: 
        if max_score == min_score:
            return 0.0
        else:
            return 0.5 - 0.5*(score/min_score)    
  
      
def getRGB (c_tuple):
    return "#%02x%02x%02x"%(int(c_tuple[0]*255), int(c_tuple[1]*255), int(c_tuple[2]*255))

     
def span_word (word, score, colormap):
    return "<span style=\"background-color:"+getRGB(colormap(score))+"\">"+word+"</span>"

def html_heatmap (words, scores, cmap_name="bwr"):
    """
    Return word-level heatmap in HTML format,
    with words being the list of words (as string),
    scores the corresponding list of word-level relevance values,
    and cmap_name the name of the matplotlib diverging colormap.
    """
    
    colormap  = plt.get_cmap(cmap_name)
     
    assert len(words)==len(scores)
    max_s     = max(scores)
    min_s     = min(scores)
    
    output_text = ""
    
    for idx, w in enumerate(words):
        score       = rescale_score_by_abs(scores[idx], max_s, min_s)
        output_text = output_text + span_word(w, score, colormap) + " "
    
    return output_text + "\n"
